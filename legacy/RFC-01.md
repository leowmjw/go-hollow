# RFC-01: Adopting Apache Arrow & Iceberg for Hollow-Go Data Representation

| Status | Draft |
|--------|-------|
| Author | Gemini (AI) |
| Stakeholders | Core Hollow-Go maintainers, Data Platform team, SRE, Performance SIG |
| Created | 2025-07-20 |
| Target Release | v0.5.0 |

---

## 1. Abstract
This RFC evaluates replacing the current JSON-encoded `map[string]any` snapshot/delta payloads with columnar Apache Arrow in memory and Apache Iceberg on S3 for long-term storage. The goal is to achieve sub-5 Âµs p99 read latency, minimise allocations, and provide strong schema typing while maintaining Hollowâ€™s read-only semantics.

## 2. Background & Motivation
The existing implementation serialises snapshots with Goâ€™s `encoding/json`, incurring:
* Excessive CPU from reflection & UTF-8 validation.
* Heap pressure from string keys and interface boxing.
* Lack of explicit schema â†’ run-time type assertions.

Apache Arrow is a language-agnostic, in-memory columnar format offering:
* Zero-copy reads via contiguous buffers.
* Strongly-typed record batches.
* SIMD-friendly vector operations.

Persisting Arrow batches in Apache Iceberg tables enables:
* Schema evolution & partitioning.
* ACID data-lake semantics on S3.
* Interop with Spark/Trino.

## 3. Proposal
1. **In-Memory Layout**
   * Replace `readState` map with Arrow `RecordBatch` backed by Go memory mapped buffers.
   * Maintain an Arrow Schema generated from Hollow type definitions.
2. **Serialisation Path**
   * **Snapshot**: Arrow IPC (Feather V2) framed blobs.
   * **Delta**: Arrow Flight-like streaming of record batch diffs.
3. **Persistence**
   * Stage blobs directly to S3 as Iceberg `data` files using [`arrowport`](https://github.com/TFMV/arrowport) writers.
   * Metastore integration deferred; producer emits Iceberg `manifest` JSON per cycle.
4. **Libraries**
   * [`github.com/apache/arrow/go/v17/arrow`]
   * [`github.com/TFMV/archery`] â€“ high-level builder APIs.
   * [`github.com/TFMV/porter`] â€“ Arrow IPC streaming helpers.

## 4. Detailed Design
### 4.1 Producer Cycle
```
WriteState â†’ Arrow Builder â†’ RecordBatch â†’ IPC â†’ BlobStager
```
* Builders allocate once per cycle, re-used across records.
* Metrics: rows, batch size, build latency.

### 4.2 Consumer Refresh
```
BlobRetriever â†’ mmap IPC file â†’ arrow/ipc.Reader â†’ atomic.Pointer[*RecordBatch]
```
* No JSON unmarshal; fields accessed via Arrow column vectors.
* Look-ups: binary search on dictionary-encoded columns or hash index (optional).

## 5. Expected Benefits
| Dimension | JSON (today) | Arrow (proposed) |
|-----------|--------------|------------------|
| p99 read latency | ~70 Âµs (est.) | â‰¤ 5 Âµs (vector load + SIMD) |
| Memory overhead | ~3Ã— raw data | â‰¤ 1.1Ã— (shared buffers) |
| Type safety | none | compile-time via schema |
| Interop | Go-only | Arrow Flight, Python, Java, Rust |

## 6. Drawbacks & Risks (Devilâ€™s Advocate)
1. **Complexity Explosion** â€“ Arrow & Iceberg introduce steep learning curves; contributors must understand columnar memory management.
2. **Binary Compatibility** â€“ Arrow format upgrades (v12â†’v13) may break consumers.
3. **GC Edge Cases** â€“ JNI-style CGo or unsafe pointers may cause leaks or seg-faults if reference counts mis-managed.
4. **Binary Size** â€“ Arrow libs add ~15 MB to binaries; not ideal for serverless.
5. **Delta Semantics** â€“ Hollowâ€™s object graph diff is row-oriented; projecting to columnar deltas may require rewriting diff engine.
6. **Iceberg Overhead** â€“ Manifest + metadata files per cycle increase small-file count on S3, impacting list operations.
7. **On-Disk Footprint** â€“ Arrow IPC is not as storage-efficient as Parquet; may bloat snapshots.
8. **Vendor Lock-In** â€“ TFMV libs are alpha; limited community support compared with core Apache repos.

## 7. Mitigations
* Start with **optional experimental path** behind feature flag.
* Retain JSON snapshot compatibility for rollback.
* Use pure-Go Arrow (`net/godoc`) avoiding CGO where possible.
* Ship memory-usage & latency SLO dashboards.
* Contribute fixes upstream to Arrow-Go project.

## 8. Alternatives Considered
1. **FlatBuffers** â€“ fast but lacks columnar projection and analytics interop.
2. **Capâ€™n Proto** â€“ zero-copy, strong typing, but poor ecosystem for analytics + S3 table formats.
3. **Protobuf w/ GZIP** â€“ smaller on disk, still requires copy into structs.
4. **Stay with JSON + Code-gen** â€“ easier, but unlikely to hit 5 Âµs target.

## 9. Decision Matrix
| Option | Read p99 | Writer CPU | Ecosystem | Complexity | Verdict |
|--------|----------|-----------|-----------|------------|---------|
| Arrow  | ðŸŸ¢ | ðŸŸ¢ | ðŸŸ¢ | ðŸ”´ | Candidate |
| FlatBuffers | ðŸŸ¢ | ðŸŸ¢ | ðŸŸ¡ | ðŸŸ¡ | Disfavoured |
| Capâ€™n Proto | ðŸŸ¢ | ðŸŸ¡ | ðŸ”´ | ðŸŸ¡ | Disfavoured |
| JSON (current) | ðŸ”´ | ðŸ”´ | ðŸŸ¢ | ðŸŸ¢ | Reject |

GreenðŸŸ¢ good, YellowðŸŸ¡ moderate, RedðŸ”´ poor.

## 10. Recommendation
Proceed with a **prototype** Arrow snapshot path focused on read-latency benchmarks. Gate behind `ARROW_EXPERIMENT=1` env var. Re-evaluate after:
* 1 M record dataset shows â‰¤ 5 Âµs p99 on `c6i.large`.
* Memory overhead <1.2Ã— raw data.
* Integration tests pass for fallback JSON path.

## 11. Unresolved Questions
* Best strategy for dictionary encoding dynamic strings?
* Influence on hot-reload developer loop (mmap semantics on macOS)?
* Iceberg commit atomicity without metastore?

## 12. References
* Archery: <https://github.com/TFMV/archery>
* Porter: <https://github.com/TFMV/porter>
* ArrowPort: <https://github.com/TFMV/arrowport>
* Apache Iceberg Go: <https://github.com/apache/iceberg-go>
* Arrow Flight + Iceberg article: <https://medium.com/@tglawless/apache-iceberg-apache-arrow-flight-7f95271b7a85>
* Arrow Columnar Format Spec: <https://arrow.apache.org/docs/format/Columnar.html>

---

*End of RFC-01*
